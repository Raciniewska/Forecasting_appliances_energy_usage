train_rsme=append(train_rsme, ret$RMSE)
# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
ret<-eval_results(y_test, predictions_test, test)
test_r2=append(test_r2, ret$Rsquare)
test_rsme=append(test_rsme, ret$RMSE)
}
plot(seq(-10,0,1) ,train_r2 ,pch=19, xlab="log10(thresh)", type="l")
plot(seq(-10,0,1) ,test_r2 ,pch=19, xlab="log10(thresh)", type="l")
plot(seq(-10,0,1) ,test_rsme ,pch=19, xlab="log10(thresh)", type="l")
plot(seq(-10,0,1) ,train_rsme ,pch=19, xlab="log10(thresh)", type="l")
library(glmnet)
train_attributes <- subset(train, select = -c(logAppliances, RH_out, RH_6, T1, X,logT5,lights) )
library(glmnet)
train_attributes <- subset(train, select = -c(logAppliances, RH_out, RH_6, T1, logT5,lights) )
library(glmnet)
train_attributes <- subset(train, select = -c(logAppliances, RH_out, RH_6, T1, logT5) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances, RH_out, RH_6, T1, X,logT5) )
x = as.matrix(train_attributes)
y_train = train$logAppliances
x_test = as.matrix(test_attributes)
y_test = test$logAppliances
lambdas <- 10^seq(2, -3, by = -.05)
cv_ridge <- cv.glmnet(x, y_train, alpha = 0, lambda = lambdas)
optimal_lambda <- cv_ridge$lambda.min
optimal_lambda
plot(lambdas,cv_ridge$cvm,ylab="Mean-Squared Error",xlab="Lambda",  type="l")
ridge_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = optimal_lambda,thresh = 1e-07)
summary(ridge_reg)
coef(ridge_reg)
# Prediction and evaluation on train data
predictions_train <- predict(ridge_reg, s = optimal_lambda, newx = x)
res<-eval_results(y_train, predictions_train, train)
print(res)
# Prediction and evaluation on test data
predictions_test <- predict(ridge_reg, s = optimal_lambda, newx = x_test)
res<-eval_results(y_test, predictions_test, test)
print(res)
train<-read.csv("../data/energy_data_train.csv")
train <- subset(train, select = -c(lights,X) )
train_attributes <- subset(train, select = -c(logAppliances) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances,x) )
train<-read.csv("../data/energy_data_train.csv")
train <- subset(train, select = -c(lights,X) )
train_attributes <- subset(train, select = -c(logAppliances) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances,X) )
logAppliances<-train$logAppliances
train <- cbind(train_attributes, logAppliances)
library(glmnet)
x = as.matrix(train_attributes)
y_train = train$logAppliances
x_test = as.matrix(test_attributes)
y_test = test$logAppliances
lambdas <- 10^seq(0, -3, by = -.05)
cv_lasso <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas)
optimal_lambda <- cv_lasso$lambda.min
optimal_lambda
plot(lambdas,cv_lasso$cvm,ylab="Mean-Squared Error",xlab="Lambda",  type="l")
lasso_reg = glmnet(x, y_train,  alpha = 1, family = 'gaussian', lambda = optimal_lambda,thresh = 1e-07)
summary(lasso_reg)
coef(lasso_reg)
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
return(data.frame(
RMSE = RMSE,
Rsquare = R_square
))}
# Prediction and evaluation on train data
predictions_train <- predict(lasso_reg, s = optimal_lambda, newx = x)
res<-eval_results(y_train, predictions_train, train)
print(res)
# Prediction and evaluation on test data
predictions_test <- predict(lasso_reg, s = optimal_lambda, newx = x_test)
res<-eval_results(y_test, predictions_test, test)
print(res)
thresh_vals = list(10^(-10),10^(-9),10^(-8),10^(-7),10^(-6), 10^(-5), 10^(-4), 10^(-3), 10^(-2), 10^(-1), 10^(-0))
train_rsme=list()
test_rsme=list()
test_r2=list()
train_r2=list()
for(i in 1:length(thresh_vals)){
lasso_reg = glmnet(x, y_train, nlambda = 25, alpha = 0, family = 'gaussian', lambda = optimal_lambda,thresh = thresh_vals[i])
# Prediction and evaluation on train data
predictions_train <- predict(lasso_reg, s = optimal_lambda, newx = x)
ret<-eval_results(y_train, predictions_train, train)
train_r2=append(train_r2, ret$Rsquare)
train_rsme=append(train_rsme, ret$RMSE)
# Prediction and evaluation on test data
predictions_test <- predict(lasso_reg, s = optimal_lambda, newx = x_test)
ret<-eval_results(y_test, predictions_test, test)
test_r2=append(test_r2, ret$Rsquare)
test_rsme=append(test_rsme, ret$RMSE)
}
plot(seq(-10,0,1) ,train_r2 ,pch=19, xlab="log10(thresh)", type="l")
plot(seq(-10,0,1) ,test_r2 ,pch=19, xlab="log10(thresh)", type="l")
plot(seq(-10,0,1) ,test_rsme ,pch=19, xlab="log10(thresh)", type="l")
plot(seq(-10,0,1) ,train_rsme ,pch=19, xlab="log10(thresh)", type="l")
library(glmnet)
train_attributes <- subset(train, select = -c(logAppliances,  RH_out, RH_6, T1, logT5) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances,RH_out, RH_6, T1, logT5,X) )
x = as.matrix(train_attributes)
y_train = train$logAppliances
x_test = as.matrix(test_attributes)
y_test = test$logAppliances
lambdas <- 10^seq(0, -3, by = -.05)
cv_lasso <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas)
optimal_lambda <- cv_lasso$lambda.min
optimal_lambda
plot(lambdas,cv_lasso$cvm,ylab="Mean-Squared Error",xlab="Lambda",  type="l")
lasso_reg = glmnet(x, y_train, nlambda = 25, alpha = 1, family = 'gaussian', lambda = optimal_lambda,thresh = 1e-07)
summary(lasso_reg)
coef(lasso_reg)
# Prediction and evaluation on train data
predictions_train <- predict(lasso_reg, s = optimal_lambda, newx = x)
res<-eval_results(y_train, predictions_train, train)
print(res)
# Prediction and evaluation on test data
predictions_test <- predict(lasso_reg, s = optimal_lambda, newx = x_test)
res<-eval_results(y_test, predictions_test, test)
print(res)
library(glmnet)
train_attributes <- subset(train, select = -c(logAppliances,  RH_out, RH_6, T1, logT5) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances,RH_out, RH_6, T1, logT5,X) )
x = as.matrix(train_attributes)
y_train = train$logAppliances
x_test = as.matrix(test_attributes)
y_test = test$logAppliances
lambdas <- 10^seq(0, -3, by = -.05)
cv_lasso <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas)
optimal_lambda <- cv_lasso$lambda.min
optimal_lambda
plot(lambdas,cv_lasso$cvm,ylab="Mean-Squared Error",xlab="Lambda",  type="l")
lasso_reg = glmnet(x, y_train, nlambda = 25, alpha = 1, family = 'gaussian', lambda = optimal_lambda,thresh = 1e-07)
summary(lasso_reg)
coef(lasso_reg)
# Prediction and evaluation on train data
#predictions_train <- predict(lasso_reg, s = optimal_lambda, newx = x)
#res<-eval_results(y_train, predictions_train, train)
#print(res)
# Prediction and evaluation on test data
predictions_test <- predict(lasso_reg, s = optimal_lambda, newx = x_test)
res<-eval_results(y_test, predictions_test, test)
print(res)
train<-read.csv("../data/energy_data_train.csv")
train <- subset(train, select = -c(lights) )
train_attributes <- subset(train, select = -c(logAppliances, X) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances, X) )
logAppliances<-train$logAppliances
train <- cbind(train_attributes, logAppliances)
library(randomForest)
set.seed(1)
#fit the random forest model
model <- randomForest(
formula = logAppliances ~ .,
data = train
)
#display fitted model
model
plot(model)
#find number of trees that produce lowest test MSE
which.min(model$mse)
#find RMSE of best model
sqrt(model$mse[which.min(model$mse)])
varImpPlot(model, pch=19)
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
return(data.frame(
RMSE = RMSE,
Rsquare = R_square
))}
#treningowe dane
y_pred_train = predict(model, newdata = train_attributes)
res<-eval_results(train$logAppliances, y_pred_train, train)
print(res)
#testowe dane
y_pred_test = predict(model, newdata = test_attributes)
res<-eval_results(test$logAppliances,y_pred_test, test)
print(res)
library(ranger)
# hyperparameter grid search
hyper_grid <- expand.grid(
mtry       = seq(3, 17, by = 2),
node_size  = seq(6, 12, by = 2),
replace = c(TRUE, FALSE),
ntry = seq(500,600,by=100),
OOB_RMSE   = 0
)
# total number of combinations
nrow(hyper_grid)
library(ranger)
# hyperparameter grid search
hyper_grid <- expand.grid(
mtry       = seq(6, 17, by = 2),
node_size  = seq(3, 9, by = 2),
replace = c(TRUE, FALSE),
ntry = seq(500,600,by=100),
OOB_RMSE   = 0
)
# total number of combinations
nrow(hyper_grid)
library(ranger)
# hyperparameter grid search
hyper_grid <- expand.grid(
mtry       = seq(3, 17, by = 2),
node_size  = seq(3, 9, by = 2),
replace = c(TRUE, FALSE),
ntry = seq(500,600,by=100),
OOB_RMSE   = 0
)
# total number of combinations
nrow(hyper_grid)
for(i in 1:nrow(hyper_grid)) {
# train model
model <- ranger(
formula         = logAppliances ~ .,
data            = train,
num.trees       = hyper_grid$ntry[i],
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$node_size[i],
replace         = hyper_grid$replace[i],
seed            = 1
)
# add OOB error to grid
hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
print(i)
}
hyper_grid <- hyper_grid[order(hyper_grid$OOB_RMSE),]
print(hyper_grid)
set.seed(1)
library(randomForest)
#fit the random forest model
model <- randomForest(
formula = logAppliances ~ .,
data = train,
replace=TRUE,
mtry=3,
ntree=600,
nodesize=3
)
#display fitted model
model
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
return(data.frame(
RMSE = RMSE,
Rsquare = R_square
))}
#treningowe dane
y_pred_train = predict(model, newdata = train_attributes)
res<-eval_results(train$logAppliances, y_pred_train, train)
print(res)
#testowe dane
y_pred_test = predict(model, newdata = test_attributes)
res<-eval_results(test$logAppliances,y_pred_test, test)
print(res)
train<-read.csv("../data/energy_data_train.csv")
train <- subset(train, select = -c(lights,X) )
train_attributes <- subset(train, select = -c(logAppliances) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances,X) )
logAppliances<-train$logAppliances
train <- cbind(train_attributes, logAppliances)
library(e1071)
model = svm(logAppliances ~ ., data = train)
print(model)
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
return(data.frame(
RMSE = RMSE,
Rsquare = R_square
))}
#treningowe dane
y_pred_train = predict(model, newdata = train_attributes)
res<-eval_results(train$logAppliances, y_pred_train, train)
print(res)
#testowe dane
y_pred_test = predict(model, newdata = test_attributes)
res<-eval_results(test$logAppliances,y_pred_test, test)
print(res)
model <- svm(logAppliances ~ ., data = train,
gamma = 0.25,
epsilon =0.1,
cost = 81)
y_pred_train = predict(model, newdata = train_attributes)
#treningowe dane
y_pred_train = predict(model, newdata = train_attributes)
res<-eval_results(train$logAppliances, y_pred_train, train)
print(res)
#testowe dane
y_pred_test = predict(model, newdata = test_attributes)
res<-eval_results(test$logAppliances,y_pred_test, test)
print(res)
train<-read.csv("../data/energy_data_train.csv")
train <- subset(train, select = -c(logAppliances,X) )
train_attributes <- subset(train, select = -c(lights) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances, X) )
lights<-train$lights
train <- cbind(train_attributes, lights)
library(randomForest)
set.seed(1)
#fit the random forest model
model <- randomForest(
formula = lights ~ .,
data = train
)
#display fitted model
model
plot(model)
#find number of trees that produce lowest test MSE
which.min(model$mse)
#find RMSE of best model
sqrt(model$mse[which.min(model$mse)])
varImpPlot(model, pch=19)
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
return(data.frame(
RMSE = RMSE,
Rsquare = R_square
))}
#treningowe dane
y_pred_train = predict(model, newdata = train_attributes)
res<-eval_results(train$lights, y_pred_train, train)
print(res)
#testowe dane
y_pred_test = predict(model, newdata = test_attributes)
res<-eval_results(test$lights,y_pred_test, test)
print(res)
set.seed(1)
library(randomForest)
#fit the random forest model
model <- randomForest(
formula = lights ~ .,
data = train,
replace=FALSE,
mtry=5,
ntree=600,
nodesize=3
)
#display fitted model
model
eval_results <- function(true, predicted, df) {
SSE <- sum((predicted - true)^2)
SST <- sum((true - mean(true))^2)
R_square <- 1 - SSE / SST
RMSE = sqrt(SSE/nrow(df))
return(data.frame(
RMSE = RMSE,
Rsquare = R_square
))}
#treningowe dane
y_pred_train = predict(model, newdata = train_attributes)
res<-eval_results(train$lights, y_pred_train, train)
print(res)
#testowe dane
y_pred_test = predict(model, newdata = test_attributes)
res<-eval_results(test$lights,y_pred_test, test)
print(res)
set.seed(1)
library(randomForest)
#fit the random forest model
model <- randomForest(
formula = lights ~ .,
data = train,
replace=FALSE,
mtry=5,
ntree=600,
nodesize=3
)
#display fitted model
model
train<-read.csv("../data/energy_data_train.csv")
train_attributes <- subset(train, select = -c(logAppliances, day_of_year) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances,day_of_year) )
test_attributes$lights <-y_pred_test
logAppliances<-train$logAppliances
train <- cbind(train_attributes, logAppliances)
hyper_grid <- expand.grid(
mtry       = seq(3, 17, by = 2),
node_size  = seq(3, 9, by = 2),
replace = c(TRUE, FALSE),
ntry = seq(500,600,by=100),
OOB_RMSE   = 0
)
for(i in 1:nrow(hyper_grid)) {
# train model
model <- ranger(
formula         = logAppliances ~ .,
data            = train,
num.trees       = hyper_grid$ntry[i],
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$node_size[i],
replace         = hyper_grid$replace[i],
seed            = 1
)
# add OOB error to grid
hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
print(i)
}
# total number of combinations
nrow(hyper_grid)
hyper_grid <- hyper_grid[order(hyper_grid$OOB_RMSE),]
print(hyper_grid)
set.seed(1)
library(randomForest)
#fit the random forest model
model <- randomForest(
formula = logAppliances ~ .,
data = train,
replace=TRUE,
mtry=3,
ntree=600,
nodesize=3
)
#display fitted model
model
#treningowe dane
y_pred_train = predict(model, newdata = train_attributes)
res<-eval_results(train$logAppliances, y_pred_train, train)
print(res)
#testowe dane
y_pred_test = predict(model, newdata = test_attributes)
res<-eval_results(test$logAppliances,y_pred_test, test)
print(res)
plot(model)
#find number of trees that produce lowest test MSE
which.min(model$mse)
#find RMSE of best model
sqrt(model$mse[which.min(model$mse)])
varImpPlot(model, pch=19)
train<-read.csv("../data/energy_data_train.csv")
train_attributes <- subset(train, select = -c(logAppliances, X) )
test<-read.csv("../data/energy_data_test.csv")
test_attributes <- subset(test, select = -c(lights, logAppliances,X) )
test_attributes$lights <-y_pred_test
logAppliances<-train$logAppliances
train <- cbind(train_attributes, logAppliances)
hyper_grid <- expand.grid(
mtry       = seq(3, 17, by = 2),
node_size  = seq(3, 9, by = 2),
replace = c(TRUE, FALSE),
ntry = seq(500,600,by=100),
OOB_RMSE   = 0
)
for(i in 1:nrow(hyper_grid)) {
# train model
model <- ranger(
formula         = logAppliances ~ .,
data            = train,
num.trees       = hyper_grid$ntry[i],
mtry            = hyper_grid$mtry[i],
min.node.size   = hyper_grid$node_size[i],
replace         = hyper_grid$replace[i],
seed            = 1
)
# add OOB error to grid
hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
print(i)
}
# total number of combinations
nrow(hyper_grid)
hyper_grid <- hyper_grid[order(hyper_grid$OOB_RMSE),]
print(hyper_grid)
set.seed(1)
library(randomForest)
#fit the random forest model
model <- randomForest(
formula = logAppliances ~ .,
data = train,
replace=TRUE,
mtry=3,
ntree=600,
nodesize=3
)
#display fitted model
model
#treningowe dane
y_pred_train = predict(model, newdata = train_attributes)
res<-eval_results(train$logAppliances, y_pred_train, train)
print(res)
#testowe dane
y_pred_test = predict(model, newdata = test_attributes)
res<-eval_results(test$logAppliances,y_pred_test, test)
print(res)
plot(model)
#find number of trees that produce lowest test MSE
which.min(model$mse)
#find RMSE of best model
sqrt(model$mse[which.min(model$mse)])
varImpPlot(model, pch=19)
